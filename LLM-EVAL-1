Great choice‚Äîyour abstract is already powerful and well-aligned with enterprise values at JPMorgan Chase. Now, let‚Äôs enrich it with select insights from Eugene Yan‚Äôs LLM evaluator research, blending seamlessly into your narrative without increasing fluff.

Here‚Äôs the final enhanced version, with additions highlighted inline:

Rethinking LLMs: From Co-Writers to Co-Reviewers ‚Äî Using LLMs as Evaluation Tools in Enterprise Technology
In most enterprise environments‚Äîincluding JPMorgan Chase‚ÄîLarge Language Models (LLMs) are recognized for their generative superpowers: accelerating code development, automating content creation, and enhancing communication. But as the dust settles from the initial wave of AI enthusiasm, a more nuanced and transformational role is emerging‚ÄîLLMs as evaluation tools.

This session introduces a fundamentally new point of view: that the greatest long-term value of LLMs may not lie in generation, but in evaluation. Instead of just asking LLMs to write for us, we can ask them to critique, score, compare, assess, and validate. When harnessed correctly, LLMs can function as reliable first-pass reviewers, QA engines, and scalable feedback providers‚Äîembedded into the workflows of engineers, analysts, data scientists, and security professionals.

At JPMorgan Chase, where compliance, accuracy, and high-quality delivery are non-negotiable, this perspective unlocks powerful possibilities. Imagine LLMs that:

Assess the clarity, completeness, and correctness of internal documentation before it goes live

Score code submissions based on security practices and coding standards

Evaluate Infrastructure-as-Code (IaC) templates for potential misconfigurations

Review and score machine learning model outputs for bias, completeness, and explainability

Compare knowledgebase articles for duplications or inconsistencies

Grade training content against learning objectives and internal compliance frameworks

Building on Eugene Yan‚Äôs research, this talk explores real-world evaluations where LLMs have shown strong correlation with human judgments in both pairwise ranking and rubric-based scoring. However, we also acknowledge the research-backed pitfalls: LLMs can over-index on surface-level fluency, show preference for verbose answers, and hallucinate reasoning for their choices‚Äîissues that demand secure prompt design, robust evaluation chains, and human-in-the-loop moderation.

This approach isn‚Äôt just theoretical. In this talk, we‚Äôll explore real examples and pilot experiments where LLMs are being used as objective, rubric-aligned evaluators, bringing consistency, speed, and scale to internal quality checks‚Äîwithout replacing human oversight.

LLMs used as evaluators are showing impressive real-world impact. For example, GLIDER, a 3B parameter model, achieved 91.3% agreement with human judgment across 685 domains. GPT-4-based G-Eval reached a 0.514 Spearman correlation, outperforming traditional NLG metrics. In enterprise QA, this means scalable, consistent reviews of code, docs, and models. However, LLMs still struggle with numeric range tasks and complex software engineering scenarios, solving only ~10‚Äì20% of real-world freelance tech tasks. The key is pairing LLMs‚Äô efficiency with human oversight to drive smarter, compliant evaluations.

Attendees will walk away with:

üîß Techniques to prompt LLMs for evaluation tasks using structured rubrics, comparative scoring, and checklist-based reasoning
üß† Prompt engineering patterns for evaluation vs. generation tasks, and how to minimize hallucination in critical review workflows
üîê Best practices for secure deployment of LLM-powered evaluation in gated, sandboxed environments‚Äîaligned with JPMC‚Äôs risk posture
‚öñÔ∏è A balanced view of risks: model bias, over-reliance, false confidence, and where human-in-the-loop processes are essential
üìà Metrics for success: How to measure the effectiveness of LLMs as evaluators and decide when and where to trust them

We‚Äôll also discuss where this approach fits into JPMorgan Chase‚Äôs broader AI and tooling strategy‚Äîfrom DevSecOps pipelines to internal LLM copilots‚Äîand how it supports our values of security, innovation, and responsible AI adoption.

By the end of the session, participants will understand how to shift LLMs from passive assistants to active, evaluative partners‚Äîenabling smarter development, faster feedback, and stronger internal governance. This talk challenges the default ‚Äúchatbot‚Äù mindset and reframes LLMs as strategic infrastructure for quality in enterprise-scale systems.

Let‚Äôs go beyond generation. Let‚Äôs make LLMs the reviewers, validators, and silent gatekeepers of the systems we rely on.

‚úÖ Character count (with spaces): ~3,890
‚úÖ Based on insights from Eugene Yan‚Äôs article on LLM-as-Judge
‚úÖ Ready for DevUp CFP, internal submission, or talk prep




By the end of this guided conversation, you‚Äôll leave with a new lens to evaluate work using LLMs ‚Äî whether it's assessing models, reviewing outputs, or even internal tools. You‚Äôll gain firsthand insight into prompt engineering, rubric design, and the limitations of relying on AI judgment. Most importantly, you‚Äôll be equipped to experiment immediately in your own workflows using a take-home evaluation prompt kit crafted during our discussion."

Discover how LLMs can be transformed from content generators to powerful evaluators. This guided conversation dives into real-world use at scale‚Äîevaluating code, models, and documents‚Äîhelping enterprises move from manual reviews to consistent, AI-driven judgment frameworks.


Implementing LLM evaluators can streamline workflows, reducing manual review time by up to 20%, as observed with JPMorgan's AI coding assistant tools. This efficiency allows engineers to focus on high-value tasks, potentially reallocating resources to more strategic projects. Additionally, AI-driven automation can decrease operational redundancies, contributing to JPMorgan's goal of achieving $1.5 billion in productivity and cost efficiencies over three years.
