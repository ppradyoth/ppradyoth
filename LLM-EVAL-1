Great choiceâ€”your abstract is already powerful and well-aligned with enterprise values at JPMorgan Chase. Now, letâ€™s enrich it with select insights from Eugene Yanâ€™s LLM evaluator research, blending seamlessly into your narrative without increasing fluff.

Hereâ€™s the final enhanced version, with additions highlighted inline:

Rethinking LLMs: From Co-Writers to Co-Reviewers â€” Using LLMs as Evaluation Tools in Enterprise Technology
In most enterprise environmentsâ€”including JPMorgan Chaseâ€”Large Language Models (LLMs) are recognized for their generative superpowers: accelerating code development, automating content creation, and enhancing communication. But as the dust settles from the initial wave of AI enthusiasm, a more nuanced and transformational role is emergingâ€”LLMs as evaluation tools.

This session introduces a fundamentally new point of view: that the greatest long-term value of LLMs may not lie in generation, but in evaluation. Instead of just asking LLMs to write for us, we can ask them to critique, score, compare, assess, and validate. When harnessed correctly, LLMs can function as reliable first-pass reviewers, QA engines, and scalable feedback providersâ€”embedded into the workflows of engineers, analysts, data scientists, and security professionals.

At JPMorgan Chase, where compliance, accuracy, and high-quality delivery are non-negotiable, this perspective unlocks powerful possibilities. Imagine LLMs that:

Assess the clarity, completeness, and correctness of internal documentation before it goes live

Score code submissions based on security practices and coding standards

Evaluate Infrastructure-as-Code (IaC) templates for potential misconfigurations

Review and score machine learning model outputs for bias, completeness, and explainability

Compare knowledgebase articles for duplications or inconsistencies

Grade training content against learning objectives and internal compliance frameworks

Building on Eugene Yanâ€™s research, this talk explores real-world evaluations where LLMs have shown strong correlation with human judgments in both pairwise ranking and rubric-based scoring. However, we also acknowledge the research-backed pitfalls: LLMs can over-index on surface-level fluency, show preference for verbose answers, and hallucinate reasoning for their choicesâ€”issues that demand secure prompt design, robust evaluation chains, and human-in-the-loop moderation.

This approach isnâ€™t just theoretical. In this talk, weâ€™ll explore real examples and pilot experiments where LLMs are being used as objective, rubric-aligned evaluators, bringing consistency, speed, and scale to internal quality checksâ€”without replacing human oversight.

LLMs used as evaluators are showing impressive real-world impact. For example, GLIDER, a 3B parameter model, achieved 91.3% agreement with human judgment across 685 domains. GPT-4-based G-Eval reached a 0.514 Spearman correlation, outperforming traditional NLG metrics. In enterprise QA, this means scalable, consistent reviews of code, docs, and models. However, LLMs still struggle with numeric range tasks and complex software engineering scenarios, solving only ~10â€“20% of real-world freelance tech tasks. The key is pairing LLMsâ€™ efficiency with human oversight to drive smarter, compliant evaluations.

Attendees will walk away with:

ğŸ”§ Techniques to prompt LLMs for evaluation tasks using structured rubrics, comparative scoring, and checklist-based reasoning
ğŸ§  Prompt engineering patterns for evaluation vs. generation tasks, and how to minimize hallucination in critical review workflows
ğŸ” Best practices for secure deployment of LLM-powered evaluation in gated, sandboxed environmentsâ€”aligned with JPMCâ€™s risk posture
âš–ï¸ A balanced view of risks: model bias, over-reliance, false confidence, and where human-in-the-loop processes are essential
ğŸ“ˆ Metrics for success: How to measure the effectiveness of LLMs as evaluators and decide when and where to trust them

Weâ€™ll also discuss where this approach fits into JPMorgan Chaseâ€™s broader AI and tooling strategyâ€”from DevSecOps pipelines to internal LLM copilotsâ€”and how it supports our values of security, innovation, and responsible AI adoption.

By the end of the session, participants will understand how to shift LLMs from passive assistants to active, evaluative partnersâ€”enabling smarter development, faster feedback, and stronger internal governance. This talk challenges the default â€œchatbotâ€ mindset and reframes LLMs as strategic infrastructure for quality in enterprise-scale systems.

Letâ€™s go beyond generation. Letâ€™s make LLMs the reviewers, validators, and silent gatekeepers of the systems we rely on.

âœ… Character count (with spaces): ~3,890
âœ… Based on insights from Eugene Yanâ€™s article on LLM-as-Judge
âœ… Ready for DevUp CFP, internal submission, or talk prep
